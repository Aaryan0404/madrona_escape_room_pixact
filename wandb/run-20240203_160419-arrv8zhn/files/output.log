TrainConfig:
  num_updates: 1000
  steps_per_update: 40
  num_bptt_chunks: 8
  lr: 0.0001
  gamma: 0.998
  ppo:
    num_mini_batches: 1
    clip_coef: 0.2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    max_grad_norm: 0.5
    num_epochs: 2
    clip_value_loss: False
    adaptive_entropy: True
  gae_lambda: 0.95
  normalize_advantages: True
  normalize_values: True
  value_normalizer_decay: 0.999
  mixed_precision: True
Update: 1
    Loss:  2.347e-01, A: -1.740e-04, V:  4.986e-01, E: -1.442e+00
    Rewards          => Avg:  1.524e-03, Min: -5.001e-03, Max:  6.134e-02
    Values           => Avg:  0.000e+00, Min:  0.000e+00, Max:  0.000e+00
    Advantages       => Avg:  6.385e-04, Min: -8.167e-02, Max:  1.207e-01
    Bootstrap Values => Avg:  0.000e+00, Min:  0.000e+00, Max:  0.000e+00
    Returns          => Avg: 1.3887882232666016e-05, σ: 1.0068359375
    Value Normalizer => Mean:  6.383e-04, σ: 5.716e-02
    FPS: 19, Update Time: 2.13, Avg FPS: 19
    PyTorch Memory Usage: 0.156GB (Reserved), 0.119GB (Current)
    Update Iter Timing     => CPU: 2.126
      Collect Rollouts     => CPU: 1.732
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 1.270, GPU: 1.265
        Simulator Step     => CPU: 0.103
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.008
      PPO                  => CPU: 0.386
        Gather Minibatch   => CPU: 0.042, GPU: 0.043
        AC Forward         => CPU: 0.142, GPU: 0.144
          rnn.fwd_sequence => CPU: 0.005
        Optimize           => CPU: 0.157
Update: 10
    Loss:  1.997e-01, A: -1.628e-04, V:  4.285e-01, E: -1.442e+00
    Rewards          => Avg: -4.929e-03, Min: -5.001e-03, Max:  8.869e-04
    Values           => Avg: -6.775e-02, Min: -6.775e-02, Max: -6.769e-02
    Advantages       => Avg: -2.693e-02, Min: -7.465e-02, Max:  6.274e-02
    Bootstrap Values => Avg: -6.775e-02, Min: -6.775e-02, Max: -6.769e-02
    Returns          => Avg: -0.615478515625, σ: 0.849609375
    Value Normalizer => Mean: -6.699e-02, σ: 4.622e-02
    FPS: 32, Update Time: 1.24, Avg FPS: 27
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.495
      Collect Rollouts     => CPU: 1.384
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.952, GPU: 0.969
        Simulator Step     => CPU: 0.117
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.004
      PPO                  => CPU: 0.106
        Gather Minibatch   => CPU: 0.033, GPU: 0.034
        AC Forward         => CPU: 0.031, GPU: 0.030
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.032
Update: 20
    Loss:  2.778e-01, A:  4.572e-04, V:  5.835e-01, E: -1.442e+00
    Rewards          => Avg: -3.082e-03, Min: -5.001e-03, Max:  3.214e-02
    Values           => Avg: -9.119e-02, Min: -9.119e-02, Max: -9.119e-02
    Advantages       => Avg:  8.308e-03, Min: -7.050e-02, Max:  8.618e-02
    Bootstrap Values => Avg: -9.119e-02, Min: -9.119e-02, Max: -9.119e-02
    Returns          => Avg: 0.0388946533203125, σ: 0.916015625
    Value Normalizer => Mean: -8.478e-02, σ: 4.984e-02
    FPS: 27, Update Time: 1.49, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.470
      Collect Rollouts     => CPU: 1.376
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.941, GPU: 0.958
        Simulator Step     => CPU: 0.117
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.004
      PPO                  => CPU: 0.091
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.025, GPU: 0.024
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.026
Update: 30
    Loss:  1.901e-01, A: -2.130e-04, V:  4.094e-01, E: -1.442e+00
    Rewards          => Avg: -3.397e-03, Min: -5.001e-03, Max:  4.904e-02
    Values           => Avg: -9.753e-02, Min: -9.753e-02, Max: -9.753e-02
    Advantages       => Avg: -3.433e-03, Min: -6.982e-02, Max:  9.253e-02
    Bootstrap Values => Avg: -9.753e-02, Min: -9.753e-02, Max: -9.753e-02
    Returns          => Avg: -0.148681640625, σ: 0.88818359375
    Value Normalizer => Mean: -9.356e-02, σ: 5.031e-02
    FPS: 26, Update Time: 1.52, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.468
      Collect Rollouts     => CPU: 1.376
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.933, GPU: 0.951
        Simulator Step     => CPU: 0.118
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.004
      PPO                  => CPU: 0.088
        Gather Minibatch   => CPU: 0.031, GPU: 0.033
        AC Forward         => CPU: 0.024, GPU: 0.022
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.025
Update: 40
    Loss:  2.109e-01, A: -8.022e-05, V:  4.508e-01, E: -1.442e+00
    Rewards          => Avg: -3.475e-03, Min: -5.001e-03, Max:  5.240e-02
    Values           => Avg: -1.058e-01, Min: -1.058e-01, Max: -1.058e-01
    Advantages       => Avg: -2.172e-03, Min: -6.866e-02, Max:  1.008e-01
    Bootstrap Values => Avg: -1.058e-01, Min: -1.058e-01, Max: -1.058e-01
    Returns          => Avg: -0.16302490234375, σ: 0.911376953125
    Value Normalizer => Mean: -9.994e-02, σ: 4.941e-02
    FPS: 27, Update Time: 1.46, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.461
      Collect Rollouts     => CPU: 1.372
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.925, GPU: 0.942
        Simulator Step     => CPU: 0.117
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.004
      PPO                  => CPU: 0.084
        Gather Minibatch   => CPU: 0.031, GPU: 0.032
        AC Forward         => CPU: 0.022, GPU: 0.021
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.023
Update: 50
    Loss:  2.443e-01, A: -6.702e-04, V:  5.189e-01, E: -1.442e+00
    Rewards          => Avg: -1.401e-03, Min: -5.001e-03, Max:  4.471e-02
    Values           => Avg: -1.122e-01, Min: -1.122e-01, Max: -1.122e-01
    Advantages       => Avg:  2.705e-02, Min: -3.012e-02, Max:  1.072e-01
    Bootstrap Values => Avg: -1.122e-01, Min: -1.122e-01, Max: -1.122e-01
    Returns          => Avg: 0.411865234375, σ: 0.657958984375
    Value Normalizer => Mean: -1.055e-01, σ: 4.957e-02
    FPS: 30, Update Time: 1.34, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.461
      Collect Rollouts     => CPU: 1.376
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.924, GPU: 0.941
        Simulator Step     => CPU: 0.117
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.004
      PPO                  => CPU: 0.081
        Gather Minibatch   => CPU: 0.030, GPU: 0.031
        AC Forward         => CPU: 0.021, GPU: 0.020
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.022
Update: 60
    Loss:  2.788e-01, A:  5.279e-05, V:  5.864e-01, E: -1.442e+00
    Rewards          => Avg: -4.868e-03, Min: -5.001e-03, Max:  5.661e-03
    Values           => Avg: -1.171e-01, Min: -1.171e-01, Max: -1.171e-01
    Advantages       => Avg: -2.943e-03, Min: -6.635e-02, Max:  1.227e-01
    Bootstrap Values => Avg: -1.171e-01, Min: -1.171e-01, Max: -1.171e-01
    Returns          => Avg: -0.1971435546875, σ: 1.05322265625
    Value Normalizer => Mean: -1.101e-01, σ: 5.050e-02
    FPS: 25, Update Time: 1.59, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.460
      Collect Rollouts     => CPU: 1.376
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.924, GPU: 0.941
        Simulator Step     => CPU: 0.117
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.004
      PPO                  => CPU: 0.080
        Gather Minibatch   => CPU: 0.030, GPU: 0.031
        AC Forward         => CPU: 0.021, GPU: 0.020
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.022
Update: 70
    Loss:  2.667e-01, A: -4.916e-04, V:  5.633e-01, E: -1.442e+00
    Rewards          => Avg: -5.001e-03, Min: -5.001e-03, Max: -5.001e-03
    Values           => Avg: -1.219e-01, Min: -1.219e-01, Max: -1.219e-01
    Advantages       => Avg: -3.103e-03, Min: -6.567e-02, Max:  1.169e-01
    Bootstrap Values => Avg: -1.219e-01, Min: -1.219e-01, Max: -1.219e-01
    Returns          => Avg: -0.2138671875, σ: 1.03173828125
    Value Normalizer => Mean: -1.141e-01, σ: 5.135e-02
    FPS: 32, Update Time: 1.27, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.453
      Collect Rollouts     => CPU: 1.369
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.920, GPU: 0.937
        Simulator Step     => CPU: 0.116
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.004
      PPO                  => CPU: 0.080
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.021, GPU: 0.020
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.022
Update: 80
    Loss:  2.469e-01, A: -7.516e-04, V:  5.241e-01, E: -1.442e+00
    Rewards          => Avg: -4.452e-03, Min: -5.001e-03, Max:  2.075e-02
    Values           => Avg: -1.262e-01, Min: -1.262e-01, Max: -1.262e-01
    Advantages       => Avg:  4.959e-03, Min: -6.519e-02, Max:  1.212e-01
    Bootstrap Values => Avg: -1.262e-01, Min: -1.262e-01, Max: -1.262e-01
    Returns          => Avg: -0.073974609375, σ: 0.95556640625
    Value Normalizer => Mean: -1.174e-01, σ: 5.229e-02
    FPS: 24, Update Time: 1.69, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.453
      Collect Rollouts     => CPU: 1.369
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.919, GPU: 0.936
        Simulator Step     => CPU: 0.116
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.003
      PPO                  => CPU: 0.080
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.020, GPU: 0.019
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.022
Update: 90
    Loss:  2.836e-01, A:  2.177e-04, V:  5.957e-01, E: -1.442e+00
    Rewards          => Avg: -5.001e-03, Min: -5.001e-03, Max: -5.001e-03
    Values           => Avg: -1.301e-01, Min: -1.301e-01, Max: -1.301e-01
    Advantages       => Avg:  3.371e-04, Min: -6.470e-02, Max:  1.251e-01
    Bootstrap Values => Avg: -1.301e-01, Min: -1.301e-01, Max: -1.301e-01
    Returns          => Avg: -0.179443359375, σ: 1.0498046875
    Value Normalizer => Mean: -1.204e-01, σ: 5.242e-02
    FPS: 29, Update Time: 1.38, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.449
      Collect Rollouts     => CPU: 1.365
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.917, GPU: 0.934
        Simulator Step     => CPU: 0.116
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.003
      PPO                  => CPU: 0.080
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.020, GPU: 0.019
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.022
Update: 100
    Loss:  6.016e-01, A: -1.941e-04, V:  1.232e+00, E: -1.442e+00
    Rewards          => Avg: -3.237e-03, Min: -5.001e-03, Max:  4.086e-02
    Values           => Avg: -1.353e-01, Min: -1.353e-01, Max: -1.353e-01
    Advantages       => Avg:  3.140e-02, Min: -6.409e-02, Max:  1.753e-01
    Bootstrap Values => Avg: -1.353e-01, Min: -1.353e-01, Max: -1.353e-01
    Returns          => Avg: 0.36669921875, σ: 1.32861328125
    Value Normalizer => Mean: -1.233e-01, σ: 5.313e-02
    FPS: 28, Update Time: 1.43, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.445
      Collect Rollouts     => CPU: 1.362
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.916, GPU: 0.933
        Simulator Step     => CPU: 0.116
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.003
      PPO                  => CPU: 0.079
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.020, GPU: 0.019
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.021
Update: 110
    Loss:  2.995e-01, A:  8.507e-04, V:  6.261e-01, E: -1.442e+00
    Rewards          => Avg: -4.536e-03, Min: -5.001e-03, Max:  1.920e-02
    Values           => Avg: -1.394e-01, Min: -1.394e-01, Max: -1.394e-01
    Advantages       => Avg:  4.974e-03, Min: -6.348e-02, Max:  1.344e-01
    Bootstrap Values => Avg: -1.394e-01, Min: -1.394e-01, Max: -1.394e-01
    Returns          => Avg: -0.14990234375, σ: 1.060546875
    Value Normalizer => Mean: -1.264e-01, σ: 5.347e-02
    FPS: 28, Update Time: 1.44, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.443
      Collect Rollouts     => CPU: 1.360
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.915, GPU: 0.932
        Simulator Step     => CPU: 0.116
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.003
      PPO                  => CPU: 0.079
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.020, GPU: 0.019
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.021
Update: 120
    Loss:  3.194e-01, A:  5.725e-04, V:  6.666e-01, E: -1.442e+00
    Rewards          => Avg: -5.001e-03, Min: -5.001e-03, Max: -5.001e-03
    Values           => Avg: -1.447e-01, Min: -1.447e-01, Max: -1.447e-01
    Advantages       => Avg:  6.538e-03, Min: -6.287e-02, Max:  1.396e-01
    Bootstrap Values => Avg: -1.447e-01, Min: -1.447e-01, Max: -1.447e-01
    Returns          => Avg: -0.161376953125, σ: 1.08544921875
    Value Normalizer => Mean: -1.294e-01, σ: 5.413e-02
    FPS: 31, Update Time: 1.27, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.441
      Collect Rollouts     => CPU: 1.359
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.914, GPU: 0.931
        Simulator Step     => CPU: 0.116
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.003
      PPO                  => CPU: 0.079
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.020, GPU: 0.019
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.021
Update: 130
    Loss:  3.601e-01, A:  4.510e-04, V:  7.482e-01, E: -1.441e+00
    Rewards          => Avg: -4.604e-03, Min: -5.001e-03, Max:  1.352e-02
    Values           => Avg: -1.522e-01, Min: -1.522e-01, Max: -1.522e-01
    Advantages       => Avg:  1.569e-02, Min: -6.192e-02, Max:  1.472e-01
    Bootstrap Values => Avg: -1.522e-01, Min: -1.522e-01, Max: -1.522e-01
    Returns          => Avg: -0.072509765625, σ: 1.0791015625
    Value Normalizer => Mean: -1.326e-01, σ: 5.463e-02
    FPS: 27, Update Time: 1.46, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.438
      Collect Rollouts     => CPU: 1.356
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.914, GPU: 0.931
        Simulator Step     => CPU: 0.116
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.003
      PPO                  => CPU: 0.079
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.020, GPU: 0.019
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.021
Update: 140
    Loss:  4.195e-01, A: -8.516e-05, V:  8.680e-01, E: -1.440e+00
    Rewards          => Avg: -3.565e-03, Min: -5.001e-03, Max:  3.079e-02
    Values           => Avg: -1.642e-01, Min: -1.642e-01, Max: -1.642e-01
    Advantages       => Avg:  2.658e-02, Min: -5.829e-02, Max:  1.592e-01
    Bootstrap Values => Avg: -1.642e-01, Min: -1.642e-01, Max: -1.642e-01
    Returns          => Avg: -0.0260009765625, σ: 1.0244140625
    Value Normalizer => Mean: -1.361e-01, σ: 5.544e-02
    FPS: 32, Update Time: 1.24, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.437
      Collect Rollouts     => CPU: 1.354
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.913, GPU: 0.930
        Simulator Step     => CPU: 0.115
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.003
      PPO                  => CPU: 0.079
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.020, GPU: 0.019
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.021
Update: 150
    Loss:  7.027e-01, A:  2.211e-03, V:  1.430e+00, E: -1.439e+00
    Rewards          => Avg: -3.084e-03, Min: -5.001e-03, Max:  4.907e-02
    Values           => Avg: -1.750e-01, Min: -1.750e-01, Max: -1.750e-01
    Advantages       => Avg:  5.026e-02, Min: -5.692e-02, Max:  1.700e-01
    Bootstrap Values => Avg: -1.750e-01, Min: -1.750e-01, Max: -1.750e-01
    Returns          => Avg: 0.259521484375, σ: 1.23583984375
    Value Normalizer => Mean: -1.395e-01, σ: 5.697e-02
    FPS: 24, Update Time: 1.64, Avg FPS: 28
    PyTorch Memory Usage: 0.158GB (Reserved), 0.120GB (Current)
    Update Iter Timing     => CPU: 1.437
      Collect Rollouts     => CPU: 1.355
        Cache RNN state    => CPU: 0.000
        Policy Infer       => CPU: 0.914, GPU: 0.931
        Simulator Step     => CPU: 0.115
        Post Step Copy     => CPU: 0.002
        Bootstrap Values   => CPU: 0.007
      Compute Advantages   => CPU: 0.003
      PPO                  => CPU: 0.079
        Gather Minibatch   => CPU: 0.030, GPU: 0.032
        AC Forward         => CPU: 0.020, GPU: 0.019
          rnn.fwd_sequence => CPU: 0.004
        Optimize           => CPU: 0.021
Traceback (most recent call last):
  File "/home/syncthreads/mikey/madrona_escape_room_pixact/scripts/train.py", line 155, in <module>
    train(
  File "/home/syncthreads/mikey/madrona_escape_room_pixact/train_src/madrona_escape_room_learn/train.py", line 382, in train
    _update_loop(
  File "/home/syncthreads/mikey/madrona_escape_room_pixact/train_src/madrona_escape_room_learn/train.py", line 319, in _update_loop
    update_result = update_iter_fn(
                    ^^^^^^^^^^^^^^^
  File "/home/syncthreads/mikey/madrona_escape_room_pixact/train_src/madrona_escape_room_learn/train.py", line 246, in _update_iter
    rollouts = rollout_mgr.collect(amp, sim, actor_critic, value_normalizer)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/syncthreads/mikey/madrona_escape_room_pixact/train_src/madrona_escape_room_learn/rollouts.py", line 137, in collect
    actor_critic.fwd_rollout(
  File "/home/syncthreads/mikey/madrona_escape_room_pixact/train_src/madrona_escape_room_learn/actor_critic.py", line 99, in fwd_rollout
    action_dists.sample(actions_out, log_probs_out)
  File "/home/syncthreads/mikey/madrona_escape_room_pixact/train_src/madrona_escape_room_learn/action.py", line 22, in sample
    actions = [dist.sample() for dist in self.dists]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/syncthreads/mikey/madrona_escape_room_pixact/train_src/madrona_escape_room_learn/action.py", line 22, in <listcomp>
    actions = [dist.sample() for dist in self.dists]
               ^^^^^^^^^^^^^
  File "/home/syncthreads/mikey/mikey_env/lib/python3.11/site-packages/torch/distributions/categorical.py", line 132, in sample
    samples_2d = torch.multinomial(probs_2d, sample_shape.numel(), True).T
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt